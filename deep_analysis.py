import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

def analyze_critical_cases():
    """An√°lisis profundo: ¬øQu√© hace cr√≠tico a un caso?"""
    
    print("=" * 70)
    print("AN√ÅLISIS PROFUNDO: ¬øQU√â CARACTERIZA A LOS CASOS CR√çTICOS?")
    print("=" * 70)
    
    # Cargar datos
    df = pd.read_csv('Animal_Abuse.csv/Animal_Abuse.csv')
    
    # Clasificar
    binary_mapping = {
        'Tortured': 'CRITICO',
        'Chained': 'CRITICO',
        'Neglected': 'NO_CRITICO',
        'No Shelter': 'NO_CRITICO',
        'In Car': 'NO_CRITICO',
        'Noise, Barking Dog (NR5)': 'NO_CRITICO',
        'Other (complaint details)': 'NO_CRITICO'
    }
    
    df['Priority'] = df['Descriptor'].map(binary_mapping)
    criticos = df[df['Priority'] == 'CRITICO'].copy()
    no_criticos = df[df['Priority'] == 'NO_CRITICO'].copy()
    
    print(f"üìä DATOS:")
    print(f"   Casos cr√≠ticos: {len(criticos):,}")
    print(f"   Casos no cr√≠ticos: {len(no_criticos):,}")
    
    # 1. AN√ÅLISIS TEMPORAL DETALLADO
    print("\n" + "="*50)
    print("1. AN√ÅLISIS TEMPORAL DETALLADO")
    print("="*50)
    
    df['Created Date'] = pd.to_datetime(df['Created Date'], errors='coerce')
    df['Hour'] = df['Created Date'].dt.hour
    df['DayOfWeek'] = df['Created Date'].dt.dayofweek
    df['Month'] = df['Created Date'].dt.month
    
    # Horas m√°s cr√≠ticas
    print("\nüïê DISTRIBUCI√ìN POR HORA:")
    criticos['Created Date'] = pd.to_datetime(criticos['Created Date'], errors='coerce')
    no_criticos['Created Date'] = pd.to_datetime(no_criticos['Created Date'], errors='coerce')
    
    criticos['Hour'] = criticos['Created Date'].dt.hour
    no_criticos['Hour'] = no_criticos['Created Date'].dt.hour
    
    hour_criticos = criticos['Hour'].value_counts().sort_index()
    hour_no_criticos = no_criticos['Hour'].value_counts().sort_index()
    
    # Ratio cr√≠tico/no_cr√≠tico por hora
    hour_ratio = (hour_criticos / (hour_criticos + hour_no_criticos) * 100).fillna(0)
    
    print("Horas con MAYOR proporci√≥n de casos cr√≠ticos:")
    top_hours = hour_ratio.nlargest(5)
    for hour, ratio in top_hours.items():
        total_hour = hour_criticos.get(hour, 0) + hour_no_criticos.get(hour, 0)
        print(f"   {hour:2d}:00 - {ratio:.1f}% cr√≠ticos ({hour_criticos.get(hour, 0)} de {total_hour} casos)")
    
    # 2. AN√ÅLISIS GEOGR√ÅFICO DETALLADO
    print("\n" + "="*50)
    print("2. AN√ÅLISIS GEOGR√ÅFICO DETALLADO")
    print("="*50)
    
    # Borough analysis
    print("\nüèôÔ∏è POR BOROUGH:")
    borough_analysis = df.groupby(['Borough', 'Priority']).size().unstack(fill_value=0)
    borough_analysis['Total'] = borough_analysis.sum(axis=1)
    borough_analysis['Pct_Critico'] = (borough_analysis['CRITICO'] / borough_analysis['Total'] * 100).round(1)
    
    print(borough_analysis[['CRITICO', 'NO_CRITICO', 'Total', 'Pct_Critico']].sort_values('Pct_Critico', ascending=False))
    
    # Location Type analysis
    print("\nüìç POR TIPO DE UBICACI√ìN:")
    location_analysis = df.groupby(['Location Type', 'Priority']).size().unstack(fill_value=0)
    location_analysis['Total'] = location_analysis.sum(axis=1)
    location_analysis['Pct_Critico'] = (location_analysis['CRITICO'] / location_analysis['Total'] * 100).round(1)
    
    # Solo mostrar ubicaciones con suficientes casos
    location_filtered = location_analysis[location_analysis['Total'] >= 100].sort_values('Pct_Critico', ascending=False)
    print("\nUbicaciones con >100 casos, ordenadas por % cr√≠ticos:")
    print(location_filtered[['CRITICO', 'NO_CRITICO', 'Total', 'Pct_Critico']].head(10))
    
    # 3. AN√ÅLISIS DE COORDENADAS
    print("\n" + "="*50)
    print("3. AN√ÅLISIS DE COORDENADAS")
    print("="*50)
    
    coords_criticos = criticos.dropna(subset=['Latitude', 'Longitude'])
    coords_no_criticos = no_criticos.dropna(subset=['Latitude', 'Longitude'])
    
    print(f"Casos cr√≠ticos con coordenadas: {len(coords_criticos):,}/{len(criticos):,} ({len(coords_criticos)/len(criticos)*100:.1f}%)")
    print(f"Casos no cr√≠ticos con coordenadas: {len(coords_no_criticos):,}/{len(no_criticos):,} ({len(coords_no_criticos)/len(no_criticos)*100:.1f}%)")
    
    if len(coords_criticos) > 0:
        print(f"\nüìä ESTAD√çSTICAS DE UBICACI√ìN:")
        print(f"Cr√≠ticos - Lat: {coords_criticos['Latitude'].mean():.4f}¬±{coords_criticos['Latitude'].std():.4f}")
        print(f"Cr√≠ticos - Lon: {coords_criticos['Longitude'].mean():.4f}¬±{coords_criticos['Longitude'].std():.4f}")
        print(f"No cr√≠ticos - Lat: {coords_no_criticos['Latitude'].mean():.4f}¬±{coords_no_criticos['Latitude'].std():.4f}")
        print(f"No cr√≠ticos - Lon: {coords_no_criticos['Longitude'].mean():.4f}¬±{coords_no_criticos['Longitude'].std():.4f}")
    
    # 4. AN√ÅLISIS DE AGENCIAS
    print("\n" + "="*50)
    print("4. AN√ÅLISIS DE AGENCIAS")
    print("="*50)
    
    agency_analysis = df.groupby(['Agency', 'Priority']).size().unstack(fill_value=0)
    agency_analysis['Total'] = agency_analysis.sum(axis=1)
    agency_analysis['Pct_Critico'] = (agency_analysis['CRITICO'] / agency_analysis['Total'] * 100).round(1)
    
    print("üìã POR AGENCIA:")
    print(agency_analysis[['CRITICO', 'NO_CRITICO', 'Total', 'Pct_Critico']].sort_values('Total', ascending=False))
    
    # 5. AN√ÅLISIS DE RESOLUCI√ìN
    print("\n" + "="*50)
    print("5. AN√ÅLISIS DE RESOLUCI√ìN")
    print("="*50)
    
    df['Closed Date'] = pd.to_datetime(df['Closed Date'], errors='coerce')
    df['Has_Resolution'] = ~df['Closed Date'].isna()
    df['Resolution_Hours'] = (df['Closed Date'] - df['Created Date']).dt.total_seconds() / 3600
    
    print("üîÑ TASAS DE RESOLUCI√ìN:")
    resolution_by_priority = df.groupby('Priority')['Has_Resolution'].agg(['count', 'sum', 'mean'])
    resolution_by_priority.columns = ['Total_Cases', 'Resolved_Cases', 'Resolution_Rate']
    resolution_by_priority['Resolution_Rate'] = (resolution_by_priority['Resolution_Rate'] * 100).round(1)
    print(resolution_by_priority)
    
    print("\n‚è±Ô∏è TIEMPO DE RESOLUCI√ìN (horas):")
    resolution_times = df[df['Has_Resolution']].groupby('Priority')['Resolution_Hours'].describe()
    print(resolution_times[['count', 'mean', 'std', '50%', '75%', 'max']])
    
    # 6. AN√ÅLISIS DE DATOS FALTANTES
    print("\n" + "="*50)
    print("6. AN√ÅLISIS DE DATOS FALTANTES")
    print("="*50)
    
    missing_analysis = []
    key_columns = ['Incident Address', 'Incident Zip', 'Latitude', 'Longitude', 'Location Type', 'Closed Date']
    
    for col in key_columns:
        criticos_missing = criticos[col].isna().sum()
        no_criticos_missing = no_criticos[col].isna().sum()
        criticos_pct = (criticos_missing / len(criticos) * 100)
        no_criticos_pct = (no_criticos_missing / len(no_criticos) * 100)
        
        missing_analysis.append({
            'Column': col,
            'Criticos_Missing': criticos_missing,
            'Criticos_Pct': criticos_pct,
            'No_Criticos_Missing': no_criticos_missing,
            'No_Criticos_Pct': no_criticos_pct,
            'Difference': criticos_pct - no_criticos_pct
        })
    
    missing_df = pd.DataFrame(missing_analysis)
    print("üìä DATOS FALTANTES POR TIPO:")
    print(missing_df.round(2))
    
    # 7. CARACTER√çSTICAS POTENCIALES NO EXPLORADAS
    print("\n" + "="*50)
    print("7. CARACTER√çSTICAS NO EXPLORADAS")
    print("="*50)
    
    print("üîç CAMPOS CON INFORMACI√ìN TEXTUAL:")
    text_fields = ['Descriptor', 'Location Type', 'Status', 'Resolution Description', 'Community Board']
    
    for field in text_fields:
        if field in df.columns:
            unique_values = df[field].nunique()
            missing_pct = df[field].isna().sum() / len(df) * 100
            print(f"   {field}: {unique_values} valores √∫nicos, {missing_pct:.1f}% faltantes")
            
            if field == 'Location Type':
                print("      Top ubicaciones cr√≠ticas:")
                location_crit = criticos[field].value_counts().head(3)
                for loc, count in location_crit.items():
                    print(f"        - {loc}: {count} casos")
    
    # 8. AN√ÅLISIS DE PATRONES ESTACIONALES
    print("\n" + "="*50)
    print("8. AN√ÅLISIS ESTACIONAL")
    print("="*50)
    
    df['Month'] = df['Created Date'].dt.month
    df['Season'] = df['Month'].map({
        12: 'Invierno', 1: 'Invierno', 2: 'Invierno',
        3: 'Primavera', 4: 'Primavera', 5: 'Primavera',
        6: 'Verano', 7: 'Verano', 8: 'Verano',
        9: 'Oto√±o', 10: 'Oto√±o', 11: 'Oto√±o'
    })
    
    seasonal_analysis = df.groupby(['Season', 'Priority']).size().unstack(fill_value=0)
    seasonal_analysis['Total'] = seasonal_analysis.sum(axis=1)
    seasonal_analysis['Pct_Critico'] = (seasonal_analysis['CRITICO'] / seasonal_analysis['Total'] * 100).round(1)
    
    print("üåû POR ESTACI√ìN:")
    print(seasonal_analysis[['CRITICO', 'NO_CRITICO', 'Total', 'Pct_Critico']])
    
    # 9. VISUALIZACI√ìN
    print("\n" + "="*50)
    print("9. CREANDO VISUALIZACIONES")
    print("="*50)
    
    fig, axes = plt.subplots(3, 2, figsize=(15, 18))
    
    # 1. Distribuci√≥n por hora
    hour_comparison = pd.DataFrame({
        'Cr√≠ticos': hour_criticos,
        'No Cr√≠ticos': hour_no_criticos
    }).fillna(0)
    
    hour_comparison.plot(kind='bar', ax=axes[0, 0], width=0.8)
    axes[0, 0].set_title('Distribuci√≥n por Hora del D√≠a')
    axes[0, 0].set_xlabel('Hora')
    axes[0, 0].set_ylabel('N√∫mero de Casos')
    axes[0, 0].legend()
    
    # 2. Ratio cr√≠tico por hora
    hour_ratio.plot(kind='bar', ax=axes[0, 1], color='red', alpha=0.7)
    axes[0, 1].set_title('% de Casos Cr√≠ticos por Hora')
    axes[0, 1].set_xlabel('Hora')
    axes[0, 1].set_ylabel('% Cr√≠ticos')
    
    # 3. Por borough
    borough_analysis[['CRITICO', 'NO_CRITICO']].plot(kind='bar', ax=axes[1, 0], stacked=True)
    axes[1, 0].set_title('Casos por Borough')
    axes[1, 0].set_xlabel('Borough')
    axes[1, 0].tick_params(axis='x', rotation=45)
    
    # 4. % cr√≠ticos por borough
    borough_analysis['Pct_Critico'].plot(kind='bar', ax=axes[1, 1], color='orange', alpha=0.7)
    axes[1, 1].set_title('% Casos Cr√≠ticos por Borough')
    axes[1, 1].set_xlabel('Borough')
    axes[1, 1].tick_params(axis='x', rotation=45)
    
    # 5. Por estaci√≥n
    seasonal_analysis[['CRITICO', 'NO_CRITICO']].plot(kind='bar', ax=axes[2, 0])
    axes[2, 0].set_title('Casos por Estaci√≥n')
    axes[2, 0].set_xlabel('Estaci√≥n')
    axes[2, 0].tick_params(axis='x', rotation=0)
    
    # 6. Top ubicaciones cr√≠ticas
    if len(location_filtered) > 0:
        top_locations = location_filtered.head(8)
        top_locations['Pct_Critico'].plot(kind='barh', ax=axes[2, 1], color='red', alpha=0.7)
        axes[2, 1].set_title('% Cr√≠ticos por Tipo de Ubicaci√≥n')
        axes[2, 1].set_xlabel('% Casos Cr√≠ticos')
    
    plt.tight_layout()
    plt.savefig('deep_analysis_results.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("‚úì Visualizaciones guardadas como 'deep_analysis_results.png'")
    
    # 10. RECOMENDACIONES PARA MEJORAR EL MODELO
    print("\n" + "="*70)
    print("RECOMENDACIONES PARA MEJORAR EL MODELO")
    print("="*70)
    
    print("\nüéØ CARACTER√çSTICAS NUEVAS A PROBAR:")
    print("   1. Location Type espec√≠fico (algunas ubicaciones tienen >30% cr√≠ticos)")
    print("   2. Combinaciones hora-borough (interacciones)")
    print("   3. Caracter√≠sticas estacionales m√°s espec√≠ficas")
    print("   4. Densidad de casos por zona geogr√°fica")
    print("   5. An√°lisis de texto en Resolution Description")
    
    print("\nüîß T√âCNICAS AVANZADAS:")
    print("   1. Feature engineering con interacciones")
    print("   2. Algoritmos m√°s sofisticados (XGBoost, LightGBM)")
    print("   3. Ensemble methods")
    print("   4. An√°lisis de clustering geogr√°fico")
    print("   5. An√°lisis de series temporales")
    
    print("\n‚ö†Ô∏è PROBLEMAS IDENTIFICADOS:")
    print("   1. Datos faltantes significativos en coordenadas")
    print("   2. Desbalance extremo (17% vs 83%)")
    print("   3. Caracter√≠sticas actuales muy b√°sicas")
    print("   4. No aprovechamos informaci√≥n textual")
    
    return df

if __name__ == "__main__":
    data = analyze_critical_cases() 